{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssk5BSNbelCF"
      },
      "source": [
        "# Word2Vec實作\n",
        "- 字詞所代表的意義非常多元，在不同狀況下，會代表不同意思。要把多元意思用單一向量表示，則必須要進行word embedding的動作，也就是把高維向量降為低維向量的過程\n",
        "- 之前介紹過，利用分散式表示法來表達字詞向量，例如PMI、SVD..統計法..等\n",
        "- 2013年神經網路盛行後，Tomas Mikolov利用神經網路訓練方式，來獲得字詞的表達向量，獲得很棒的成果。一般認為是利用神經網路模擬人類的理解能力，獲得不錯的分布空間所得到的成果。\n",
        "- 本範例以維基百科wiki部分資料作範例\n",
        "- 資料來源：https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2\n",
        "- 利用結巴分詞(jieba)進行斷詞，gensim套件進行word2vec計算\n",
        "- 本範例約需1小時長時間執行\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dumps.wikimedia.org/zhwiki/20240501/zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2"
      ],
      "metadata": {
        "id": "ylVzFulsmgzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### opencc是繁簡轉換工具"
      ],
      "metadata": {
        "id": "eMIti7ru3701"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ta_qze2iHJi"
      },
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gensim是訓練word2vec的函式庫"
      ],
      "metadata": {
        "id": "SQCWrmFJ4Mps"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BIoy7K5eqIS"
      },
      "source": [
        "# 从 gensim 库中导入 WikiCorpus 类\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "# 使用 WikiCorpus 类加载维基百科的语料库文件，并创建一个 WikiCorpus 对象\n",
        "wiki_corpus = WikiCorpus('zhwiki-20240501-pages-articles-multistream1.xml-p1p187712.bz2', dictionary={})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T7ZZpDr3Sg8"
      },
      "source": [
        "wiki_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqX5g3Idsp-V"
      },
      "source": [
        "next(iter(wiki_corpus.get_texts()))[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 把wiki的資料檔案，轉換成連續文字的txt檔案"
      ],
      "metadata": {
        "id": "AjOTHZGR5nCE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL99YGiSfhGu"
      },
      "source": [
        "# 打开一个文本文件用于写入，编码方式为 UTF-8\n",
        "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
        "    # 遍历维基百科语料库中的每篇文章\n",
        "    for text in wiki_corpus.get_texts():\n",
        "        # 将文章中的每个词用空格连接起来，然后写入文件，并在每篇文章末尾添加换行符\n",
        "        f.write(' '.join(text)+'\\n')\n",
        "\n",
        "        # 文章数量加一\n",
        "        text_num += 1\n",
        "\n",
        "        # 每处理完 10000 篇文章就打印一次处理进度\n",
        "        if text_num % 10000 == 0:\n",
        "            print('{} articles processed.'.format(text_num))\n",
        "\n",
        "    # 打印最终的处理进度，即文章总数\n",
        "    print('{} articles processed.'.format(text_num))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ01OEi6szXr"
      },
      "source": [
        "# 导入所需库\n",
        "import jieba\n",
        "from opencc import OpenCC\n",
        "\n",
        "# 初始化 OpenCC，将繁体中文转换为简体中文\n",
        "cc = OpenCC('s2t')\n",
        "\n",
        "# 读取文件中的训练数据（繁体中文）\n",
        "train_data = open('wiki_text.txt', 'r', encoding='utf-8').read()\n",
        "\n",
        "# 将繁体中文转换为简体中文\n",
        "train_data = cc.convert(train_data)\n",
        "\n",
        "# 使用 jieba 进行分词\n",
        "train_data = jieba.lcut(train_data)\n",
        "\n",
        "# 去除空格等无效词语\n",
        "train_data = [word for word in train_data if word != '']\n",
        "\n",
        "# 将分词结果转换为字符串形式\n",
        "train_data = ' '.join(train_data)\n",
        "\n",
        "# 将处理后的文本写入到新文件中\n",
        "open('seg.txt', 'w', encoding='utf-8').write(train_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "\n",
        "# Settings\n",
        "seed = 666\n",
        "sg = 0\n",
        "window_size = 10\n",
        "#vector_size = 100\n",
        "min_count = 1\n",
        "workers = 8\n",
        "#epochs = 5\n",
        "batch_words = 10000\n",
        "\n",
        "train_data = word2vec.LineSentence('seg.txt')\n",
        "model = word2vec.Word2Vec(\n",
        "    train_data,\n",
        "    min_count=min_count,\n",
        "    #size=vector_size,\n",
        "    workers=workers,\n",
        "    #iter=epochs,\n",
        "    window=window_size,\n",
        "    sg=sg,\n",
        "    seed=seed,\n",
        "    batch_words=batch_words\n",
        ")\n",
        "\n",
        "model.save('word2vec.model')"
      ],
      "metadata": {
        "id": "_Z_DR9AvP-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXxRBbays3k0"
      },
      "source": [
        "# 导入 Word2Vec 相关模块\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# 设置 Word2Vec 训练参数\n",
        "# Settings\n",
        "seed = 666               # 随机种子\n",
        "sg = 0                   # 选择 CBOW 模型（sg=0），或者 Skip-gram 模型（sg=1）\n",
        "window_size = 10         # 上下文窗口大小\n",
        "#vector_size = 100\n",
        "min_count = 1            # 忽略词频低于 min_count 的词语\n",
        "workers = 8              # 使用多少个 CPU 核心进行训练\n",
        "#epochs = 5\n",
        "batch_words = 10000      # 每个批次包含的词语数量\n",
        "\n",
        "# 从分词后的文本文件中读取训练数据\n",
        "train_data = word2vec.LineSentence('seg.txt')\n",
        "\n",
        "# 使用 Word2Vec 模型进行训练\n",
        "model = word2vec.Word2Vec(\n",
        "    train_data,\n",
        "    min_count=min_count,\n",
        "    #size=vector_size,\n",
        "    workers=workers,\n",
        "    #iter=epochs,\n",
        "    window=window_size,\n",
        "    sg=sg,\n",
        "    seed=seed,\n",
        "    batch_words=batch_words\n",
        ")\n",
        "\n",
        "# 保存训练好的 Word2Vec 模型\n",
        "model.save('word2vec.model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PovTacQs-R_"
      },
      "source": [
        "# 导入 Word2Vec 模块\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# 要查询相似词语的词语\n",
        "string = '電腦'\n",
        "\n",
        "# 加载之前训练好的 Word2Vec 模型\n",
        "model = word2vec.Word2Vec.load('word2vec.model')\n",
        "\n",
        "# 打印要查询相似词语的原词\n",
        "print(string)\n",
        "\n",
        "# 遍历模型中与原词最相似的词语，并打印出来\n",
        "for item in model.wv.most_similar(string):\n",
        "    print(item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E235am9EVIuG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}